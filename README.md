# Clinician-Conditioned Diffusion Synthesis of Syndromic Newborn Faces (Anonymous Release)

This repository contains code to (1) fine-tune Stable Diffusion XL
(SDXL) using LoRA adapters with clinician-informed conditioning, and (2)
generate synthetic newborn face images under structured prompts.

> Important: No real patient images are included in this repository.
> Only code and synthetic samples are provided.

------------------------------------------------------------------------

## Repository Structure

    .
    ├── train_weight_text_peft_public.py      # SDXL + LoRA fine-tuning with weighted sampling
    ├── inference_scale_peft_public.py        # Synthetic image generation from trained LoRA adapters
    ├── environment.yml                       # Conda environment used in experiments
    ├── synthetic_images/                     # Sample synthetic outputs (no real data)
    │   └── README.md

------------------------------------------------------------------------

## Environment Setup

Create environment:

``` bash
conda env create -f environment.yml
conda activate <ENV_NAME>
```

------------------------------------------------------------------------

## Training

The training script `train_weight_text_peft_public.py`:

-   Loads SDXL base model (local or HF path)
-   Freezes base weights
-   Injects LoRA adapters into UNet attention layers
-   Injects LoRA adapters into both SDXL text encoders
-   Uses weighted sampling to emphasize clinically important subgroups
-   Saves LoRA adapters only (not full SDXL model)

### Example Training Command

``` bash
accelerate launch --mixed_precision bf16 --num_processes 1 train_weight_text_peft_public.py   --pretrained_model_name_or_path="/path/to/sdxl"   --instance_data_dir="/path/to/training_data"   --output_dir="/path/to/output_dir"   --instance_prompt="smartphone photo of a child, close-up face, neutral background, not retouched"   --size=512   --repeats=1   --rank=32   --train_batch_size=4   --learning_rate=5e-5   --lr_scheduler="constant_with_warmup"   --lr_warmup_steps=100   --max_train_steps=8000   --checkpointing_steps=1000   --seed=0   --gradient_checkpointing   --use_8bit_adam   --mixed_precision="bf16"
```

Outputs are saved as LoRA adapters:

    checkpoint-XXXX/
      ├── unet_lora/
      ├── te1_lora/
      └── te2_lora/

------------------------------------------------------------------------

## Inference

The script `inference_scale_peft_public.py`:

-   Loads SDXL base model
-   Loads trained LoRA adapters (UNet + both text encoders)
-   Uses DPM-Solver scheduler
-   Generates 512x512 images
-   Uses deterministic seeding
-   Supports conditioning on race and clinician-annotated features

Run:

``` bash
python inference_scale_peft_public.py
```

Images are saved in a structured directory:

    feat/<feature_slug>/<race>/syn_<race>_<feature_slug>_seed<seed>.png

------------------------------------------------------------------------

## Synthetic Images Folder

The `synthetic_images/` directory contains example synthetic newborn
images generated by this pipeline.\
No real patient data is included.

------------------------------------------------------------------------

## Responsible Use

-   For research and education only
-   Not intended for clinical decision-making
-   Synthetic outputs may contain artifacts
